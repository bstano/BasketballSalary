---
title: "Basketball Salary"
author: "Ben Stano, ADD NAMES"
date: "2/16/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Baseketballs Brooooo

```{r, message=FALSE}
library(tidyverse)
```

## Importing Data

```{r}
# players <- read_csv("../data/players.csv")
# stats.36 <- read_csv("../data/PlayerStatisticsPer36Min.csv")
# stats.100 <- read_csv("../data/PlayerStatisticsPer100Poss.csv")
# stats.g <- read_csv("../data/PlayerStatisticsPerGame.csv")
# # salary <- read_csv("../data/salaries_1985to2018.csv")
# salary2 <- read_csv("../data/nba-salaries.csv")
```

## Cleaning and Merging Data

```{r}
# stats.g <- stats.g %>%
#   mutate(name = str_replace(Player, "\\*", ""))
# stats.g <- stats.g %>%
#   mutate(key = str_c(name, as.character(year), sep = ""))
# salary2 <- salary2 %>%
#   mutate(key = str_c(name, as.character(season), sep = ""))
```

### Joining the Datasets

```{r}
# final.data <- players %>%
#   inner_join(salary2, by = "name") %>%
#   inner_join(stats.g, by = "key") %>%
#   select(-key) %>%
#   filter(year >= 2017)
# 
# write_csv(final.data, "../data/final_data.csv")

bdata.raw <- read_csv("../data/final_data.csv")
```

### Cleaning the Data

```{r}
bdata.raw <- read_csv("../data/final_data.csv")
# displaying null values in each column
bdata.raw %>%
  map_dbl(~sum(is.na(.))) %>%
  unname() ->
na.dbl

na.cols <- data.frame("variable" = colnames(bdata.raw),
                      "null.count" = na.dbl)

na.cols %>%
  filter(null.count > 0)

# parsing numbers from difficult columns
bdata.raw$career_FG3_P <- parse_number(as.character(bdata.raw$`career_FG3%`))
bdata.raw$career_FG_P <- parse_number(as.character(bdata.raw$`career_FG%`))
bdata.raw$career_FT_P <- parse_number(as.character(bdata.raw$`career_FT%`))
# cleaning height
heights <- str_split(bdata.raw$height, "\\-")
heights.df <- t(data.frame(heights))
heights.int <- as.integer(heights.df[,1]) * 12 + as.integer(heights.df[,2])
bdata.raw$height <- heights.int

bdata.raw %>%
  mutate(draft_pick = parse_number(draft_pick),
         draft_round = parse_number(draft_round),
         shoots_right = ifelse(str_detect(shoots, "Right"),1,0),
         weight = parse_number(weight),
         Pos = as.factor(Pos)) %>%
  rename(career_eFG_P = `career_eFG%`,
         FG_P = `FG%`,
         THREEP_P = `3P%`,
         THREEP = `3P`,
         THREEP_A = `3PA`,
         TWOP_P = `2P%`,
         TWOP = `2P`,
         TWOP_A = `2PA`,
         eFG_P = `eFG%`,
         FT_P = `FT%`) %>%
  select(-birthDate, -birthPlace, -college, -draft_team, -highSchool, -name.x,
         -position.x, -position.y, -team, -Player, -name.y, -season, -Tm,
         # rank removed
         -`career_FG3%`, -`career_FG%`, -`career_FT%`, -shoots, -rank) %>%
  na.omit() ->
bdata.clean

bdata.clean %>%
  select(-`_id`) ->
bdata.model
```
We chose not to use rank as a predictor


## Transforming the Response Variable

```{r}
ols.init <- lm(salary ~ ., data = bdata.model)

summary(ols.init)

plot(ols.init, which = 1)

plot(ols.init, which = 2)
```



```{r}
bdata.model %>%
  mutate(log_salary = log(salary)) -> 
bdata.log

bdata.log$salary <- NULL
```

## Removing Outliers

```{r}
ols.log0 <- lm(log_salary ~ ., data = bdata.log)

summary(ols.log0)

plot(ols.log0, which = c(1, 2))

library(car)

vif(ols.log0)
```
```{r}
outlierTest(ols.log0)

bdata.log.rm <- bdata.log[-c(774, 967, 195, 363),]
```


```{r}
ols.log <- lm(log_salary ~ ., data = bdata.log.rm)

summary(ols.log)

plot(ols.log, which = c(1, 2))

# cond.index(ols.log, bdata.log)

vif(ols.log)
```

## Feature Selection Through VIF

```{r}
vifs <- data.frame(vif(ols.log))

vifs %>%
  filter(GVIF > 1000) ->
vif.1000

drop.vars <- c(rownames(vif.1000))

bdata.log.rm %>%
  select(-drop.vars) ->
bdata.log2
```

#### VIF Refitting

```{r}
ols.log2 <- lm(log_salary ~ ., data = bdata.log2)

summary(ols.log2)

plot(ols.log2, which = c(1, 2))

vif(ols.log2)
```


```{r}
vifs2 <- data.frame(vif(ols.log2))

vifs2 %>%
  filter(GVIF > 50) ->
vif.50

drop.vars <- c(drop.vars, rownames(vif.50))

bdata.log.rm %>%
  select(-drop.vars) ->
bdata.log3
```

#### VIF Refitting Again

```{r}
ols.log3 <- lm(log_salary ~ ., data = bdata.log3)

summary(ols.log3)

plot(ols.log3, which = c(1, 2))

vif(ols.log3)
```


```{r}
vifs3 <- data.frame(vif(ols.log3))

vifs3 %>%
  filter(GVIF > 20) ->
vif.20

drop.vars <- c(drop.vars, rownames(vif.20))

bdata.log.rm %>%
  select(-drop.vars) ->
bdata.log4
```

#### VIF Refitting AGAIN AGAIN

```{r}
ols.log4 <- lm(log_salary ~ ., data = bdata.log4)

summary(ols.log4)

plot(ols.log4, which = c(1, 2))

vif(ols.log4)
```

```{r}
vifs4 <- data.frame(vif(ols.log4))

vifs4 %>%
  filter(`vif.ols.log4.` > 12) ->
vif.12

drop.vars <- c(drop.vars, rownames(vif.12))

bdata.log.rm %>%
  select(-drop.vars) ->
bdata.log5
```

#### VIF Final Fitting

```{r}
ols.log5 <- lm(log_salary ~ ., data = bdata.log5)

summary(ols.log5)

plot(ols.log5, which = c(1, 2))

vif(ols.log5)
```
### Removing High Leverage Values

```{r}
high.lev <- c()
for (i in seq(length(hatvalues(ols.log5)))) {
  if (hatvalues(ols.log5)[i] > 2*mean(hatvalues(ols.log5))){
    high.lev <- c(high.lev, i)
  }
}

bdata.log.rm <- bdata.log.rm[-high.lev,]

bdata.log.rm %>%
  select(-drop.vars) ->
bdata.vif
```

#### VIF FINAL Final Fitting

```{r}
ols.vif <- lm(log_salary ~ ., data = bdata.vif)

summary(ols.vif)

plot(ols.vif, which = c(1, 2))

vif(ols.vif)
```
```{r}
outlierTest(ols.vif)
```


## Feature Selection Use Stepwise Function

```{r}
fit.large <- lm(log_salary ~ ., data = bdata.log.rm)

fit.small <- lm(log_salary ~ 1, data = bdata.log.rm)

fit.step <- step(fit.large, fit.small)
```

### Two Feature Sets

```{r}
vars.vif <- names(vif(ols.log5))

vars.step <- names(fit.step$coefficients)[-1]
bdata.log.rm %>%
  select(c(vars.step, log_salary)) ->
bdata.step
```

## Validating Between the Feature Sets

Let's do an anova on the ols model fitting all features and the 2 feature set models

```{r}
ols.log <- lm(log_salary ~ ., data = bdata.log.rm)

anova(ols.log, ols.vif)

anova(ols.log, fit.step)
```

Both models are not significant worse than the model fit on all predictors

### Creating the Shared Feature Set

Let's do another anova on the feature set models and a small model fit only on the predictors they share

```{r}
vars.share <- c()
for (i in vars.step) {
  if (i %in% vars.vif) {
    vars.share <- c(vars.share, i)
  } else {
    print(i)
  }
}
vars.share

bdata.log.rm %>%
  select(c(vars.share, log_salary)) ->
bdata.share

ols.share <- lm(log_salary ~ ., data = bdata.share)


anova(ols.share, fit.step)

anova(ols.share, ols.vif)
```

The step variables model has slight significant anova results, but not by much.

The vif variable models is not significantly better than the shared model

Let's do CV on the step, vif, and shared feature subsets to see which is the best at prediction

### Cross Validation between Feature Sets using Leave One Out

```{r}
library(boot)

ols.share <- glm(log_salary ~ ., data = bdata.share)

ols.vif <- glm(log_salary ~ ., data = bdata.vif)

ols.step <- glm(log_salary ~ ., data = bdata.step)

cv.share <- cv.glm(bdata.share, ols.share)

cv.vif <- cv.glm(bdata.vif, ols.vif)

cv.step <- cv.glm(bdata.step, ols.step)

c("Shared" = cv.share$delta[1], "VIF" = cv.vif$delta[1], "Step" = cv.step$delta[1])
```

The Shared data set actually had better cross validation results, though the set selected by VIF was quite close.

## Weighted Least Squares Regression

```{r}
# Shares Features
glm.share <- glm(log_salary ~ ., data= bdata.share)

glm.abs.res <- glm(abs(residuals(glm.share)) ~ fitted(glm.share))

wts.share <- 1/fitted(glm.abs.res)^2

bdata.share.wls <- bdata.share

wls.share <- glm(log_salary ~ ., data = bdata.share.wls, weights = wts.share)

bdata.share.wls$wts.share <- wts.share

cv.wls.share <- cv.glm(bdata.share.wls, wls.share)
```

```{r}
# VIF Features
glm.vif <- glm(log_salary ~ ., data= bdata.vif)

glm.abs.res <- glm(abs(residuals(glm.vif)) ~ fitted(glm.vif))

wts.vif <- 1/fitted(glm.abs.res)^2

bdata.vif.wls <- bdata.vif

wls.vif <- glm(log_salary ~ ., data = bdata.vif, weights = wts.vif)

bdata.vif.wls$wts.vif <- wts.vif

cv.wls.vif <- cv.glm(bdata.vif.wls, wls.vif)
```

```{r}
# Step Features

glm.step <- glm(log_salary ~ ., data= bdata.step)

glm.abs.res <- glm(abs(residuals(glm.step))~fitted(glm.step))

wts.step <- 1/fitted(glm.abs.res)^2

bdata.step.wls <- bdata.step

wls.step <- glm(log_salary ~ ., data = bdata.step, weights=wts.step)

bdata.step.wls$wts.step <- wts.step

cv.wls.step <- cv.glm(bdata.step.wls, wls.step)
```

### CV Results between Models and Feature Sets using Leave One Out

```{r}
cbind("WLS Share" = cv.wls.share$delta, "OLS Share" = cv.share$delta,
      "WLS VIF" = cv.wls.vif$delta, "OLS VIF" = cv.vif$delta,
      "WLS Step" = cv.wls.step$delta, "OLS Step" = cv.step$delta)
```


Traditional OLS using the Shared Features is still producing the best CV results

## Regression Trees

```{r}
library(tree)

tree.share <- tree(log_salary ~ ., bdata.share, mindev = .005)

tree.vif <- tree(log_salary ~ ., bdata.vif)

tree.step <- tree(log_salary ~ ., bdata.step)

plot(tree.share)
text(tree.share)

plot(tree.vif)
text(tree.vif)

plot(tree.step)
text(tree.step)
```

# Feature Sets Summary

```{r}
print("Shared Features")
vars.share
print("Step Features")
vars.step
print("VIF Features")
vars.vif
```

# Conclusion

OLS with the Shared Feature set was the model with the best CV results

```{r}
summary(ols.step)

plot(ols.share, which = c(1, 2))
```

## Should We Remove the insignificant variables?

```{r}
bdata.step %>%
  select(-draft_pick, -Age, -G, -BLK) ->
bdata.step2

ols.step2 <- glm(log_salary ~ ., data = bdata.step2)

lm.step <- lm(log_salary ~ ., data = bdata.step)
lm.step2 <- lm(log_salary ~ ., data = bdata.step2)


anova(lm.step2, lm.step)

cv.ols.step2 <- cv.glm(bdata.step2, ols.step2)

cbind("Step" = cv.step$delta, "Step2" = cv.ols.step2$delta)
```

The ANOVA test gives us significant results, this indicates using the larger dataset.

Additionally, the larger data set cross validates better. We will keep the insignificant variables in.

# Final Model
```{r}
# library(lmtest)
# 
# bptest(ols.share2, data = bdata.share)
```

```{r}
# outlierTest(ols.share2)
```


```{r}
summary(ols.step)

plot(ols.step, which = c(1,2))
```






