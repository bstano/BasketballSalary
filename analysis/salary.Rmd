---
title: "Basketball Salary"
author: "Ben Stano, ADD NAMES"
date: "2/16/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Baseketballs Salary

```{r, message=FALSE}
library(tidyverse)
```

# Importing Data

```{r}
# players <- read_csv("../data/players.csv")
# stats.36 <- read_csv("../data/PlayerStatisticsPer36Min.csv")
# stats.100 <- read_csv("../data/PlayerStatisticsPer100Poss.csv")
# stats.g <- read_csv("../data/PlayerStatisticsPerGame.csv")
# # salary <- read_csv("../data/salaries_1985to2018.csv")
# salary2 <- read_csv("../data/nba-salaries.csv")
```

# Cleaning and Merging Data

```{r}
# stats.g <- stats.g %>%
#   mutate(name = str_replace(Player, "\\*", ""))
# stats.g <- stats.g %>%
#   mutate(key = str_c(name, as.character(year), sep = ""))
# salary2 <- salary2 %>%
#   mutate(key = str_c(name, as.character(season), sep = ""))
```

## Joining the Datasets

```{r}
# final.data <- players %>%
#   inner_join(salary2, by = "name") %>%
#   inner_join(stats.g, by = "key") %>%
#   select(-key) %>%
#   filter(year >= 2017)
# 
# write_csv(final.data, "../data/final_data.csv")

bdata.raw <- read_csv("../data/final_data.csv")
```

## Cleaning the Data

```{r}
bdata.raw <- read_csv("../data/final_data.csv")
# displaying null values in each column
bdata.raw %>%
  map_dbl(~sum(is.na(.))) %>%
  unname() ->
na.dbl

na.cols <- data.frame("variable" = colnames(bdata.raw),
                      "null.count" = na.dbl)

na.cols %>%
  filter(null.count > 0)

# parsing numbers from difficult columns
bdata.raw$career_FG3_P <- parse_number(as.character(bdata.raw$`career_FG3%`))
bdata.raw$career_FG_P <- parse_number(as.character(bdata.raw$`career_FG%`))
bdata.raw$career_FT_P <- parse_number(as.character(bdata.raw$`career_FT%`))
# cleaning height
heights <- str_split(bdata.raw$height, "\\-")
heights.df <- t(data.frame(heights))
heights.int <- as.integer(heights.df[,1]) * 12 + as.integer(heights.df[,2])
bdata.raw$height <- heights.int

bdata.raw %>%
  mutate(draft_pick = parse_number(draft_pick),
         draft_round = parse_number(draft_round),
         shoots_right = ifelse(str_detect(shoots, "Right"),1,0),
         weight = parse_number(weight),
         Pos = as.factor(Pos)) %>%
  rename(career_eFG_P = `career_eFG%`,
         FG_P = `FG%`,
         THREEP_P = `3P%`,
         THREEP = `3P`,
         THREEP_A = `3PA`,
         TWOP_P = `2P%`,
         TWOP = `2P`,
         TWOP_A = `2PA`,
         eFG_P = `eFG%`,
         FT_P = `FT%`) %>%
  select(-birthDate, -birthPlace, -college, -draft_team, -highSchool, -name.x,
         -position.x, -position.y, -team, -Player, -name.y, -season, -Tm,
         # rank removed
         -`career_FG3%`, -`career_FG%`, -`career_FT%`, -shoots, -rank) %>%
  na.omit() ->
bdata.clean

bdata.clean %>%
  select(-`_id`) ->
bdata.model
```

We chose not to use rank as a predictor


# Transforming the Response Variable

```{r}
ols.init <- lm(salary ~ ., data = bdata.model)

summary(ols.init)

plot(ols.init, which = 1)

plot(ols.init, which = 2)
```

The relationship between the response and the predictors is clearly curved

```{r}
bdata.model %>%
  mutate(log_salary = log(salary)) -> 
bdata.log

bdata.log$salary <- NULL
```


```{r}
ols.log0 <- lm(log_salary ~ ., data = bdata.log)

summary(ols.log0)

plot(ols.log0, which = c(1, 2))

library(car)

vif(ols.log0)
```

# Feature Selection and Outlier Removal

## Removing Outliers using `outlierTest()`

```{r}
outlierTest(ols.log0)

bdata.log.rm <- bdata.log[-c(774, 967, 195, 363),]
```

## Feature Selection Using VIF

```{r}
ols.log <- lm(log_salary ~ ., data = bdata.log.rm)

summary(ols.log)

plot(ols.log, which = c(1, 2))

vif(ols.log)
```


```{r}
vifs <- data.frame(vif(ols.log))

vifs %>%
  filter(GVIF > 1000) ->
vif.1000

drop.vars <- c(rownames(vif.1000))

bdata.log.rm %>%
  select(-drop.vars) ->
bdata.log2
```

### VIF Fit 2

```{r}
ols.log2 <- lm(log_salary ~ ., data = bdata.log2)

summary(ols.log2)

plot(ols.log2, which = c(1, 2))

vif(ols.log2)
```


```{r}
vifs2 <- data.frame(vif(ols.log2))

vifs2 %>%
  filter(GVIF > 50) ->
vif.50

drop.vars <- c(drop.vars, rownames(vif.50))

bdata.log.rm %>%
  select(-drop.vars) ->
bdata.log3
```

### VIF Fit 3

```{r}
ols.log3 <- lm(log_salary ~ ., data = bdata.log3)

summary(ols.log3)

plot(ols.log3, which = c(1, 2))

vif(ols.log3)
```


```{r}
vifs3 <- data.frame(vif(ols.log3))

vifs3 %>%
  filter(GVIF > 20) ->
vif.20

drop.vars <- c(drop.vars, rownames(vif.20))

bdata.log.rm %>%
  select(-drop.vars) ->
bdata.log4
```

### VIF Fit 4

```{r}
ols.log4 <- lm(log_salary ~ ., data = bdata.log4)

summary(ols.log4)

plot(ols.log4, which = c(1, 2))

vif(ols.log4)
```

```{r}
vifs4 <- data.frame(vif(ols.log4))

vifs4 %>%
  filter(`vif.ols.log4.` > 12) ->
vif.12

drop.vars <- c(drop.vars, rownames(vif.12))

bdata.log.rm %>%
  select(-drop.vars) ->
bdata.log5
```

### VIF Fit 5

```{r}
ols.log5 <- lm(log_salary ~ ., data = bdata.log5)

summary(ols.log5)

plot(ols.log5, which = c(1, 2))

vif(ols.log5)
```

## Removing High Leverage Outliers

```{r}
high.lev <- c()
for (i in seq(length(hatvalues(ols.log5)))) {
  if (hatvalues(ols.log5)[i] > 2*mean(hatvalues(ols.log5))){
    high.lev <- c(high.lev, i)
  }
}

bdata.log.rm <- bdata.log.rm[-high.lev,]

bdata.log.rm %>%
  select(-drop.vars) ->
bdata.vif
```

### VIF Final Fit

```{r}
ols.vif <- lm(log_salary ~ ., data = bdata.vif)

summary(ols.vif)

plot(ols.vif, which = c(1, 2))

vif(ols.vif)
```


## Feature Selection Use Stepwise Function

```{r}
fit.large <- lm(log_salary ~ ., data = bdata.log.rm)

fit.small <- lm(log_salary ~ 1, data = bdata.log.rm)

fit.step <- step(fit.large, fit.small)
```

## Handling the `VIF` and `Step` Feature Sets

```{r}
vars.vif <- names(vif(ols.log5))

vars.step <- names(fit.step$coefficients)[-1]
bdata.log.rm %>%
  select(c(vars.step, log_salary)) ->
bdata.step
```

## Validating Between the Feature Sets

Let's do an anova on the ols model fitting all features and the 2 feature set models

```{r}
ols.log <- lm(log_salary ~ ., data = bdata.log.rm)

anova(ols.log, ols.vif)

anova(ols.log, fit.step)
```

Both models are not significant worse than the model fit on all predictors

## Creating the `Shared` Feature Set

Let's do another anova on the feature set models and a small model fit only on the predictors they share

```{r}
vars.share <- c()
for (i in vars.step) {
  if (i %in% vars.vif) {
    vars.share <- c(vars.share, i)
  } else {
    print(i)
  }
}
vars.share

bdata.log.rm %>%
  select(c(vars.share, log_salary)) ->
bdata.share

ols.share <- lm(log_salary ~ ., data = bdata.share)


anova(ols.share, fit.step)

anova(ols.share, ols.vif)
```

The `Step` feature set model has slight significant anova results, but not by much.

The `VIF` variable models is not significantly better than the shared model

Let's do CV on the `Step`, `VIF`, and `Shared` feature subsets to see which is the best at prediction

# OLS Regression and Cross Validation on Each Feature Set

Here we will use Leave One Out Cross Validation

```{r}
library(boot)

ols.share <- glm(log_salary ~ ., data = bdata.share)

ols.vif <- glm(log_salary ~ ., data = bdata.vif)

ols.step <- glm(log_salary ~ ., data = bdata.step)

cv.share <- cv.glm(bdata.share, ols.share)

cv.vif <- cv.glm(bdata.vif, ols.vif)

cv.step <- cv.glm(bdata.step, ols.step)

c("Shared" = cv.share$delta[1], "VIF" = cv.vif$delta[1], "Step" = cv.step$delta[1])
```

The OLS using the `Step` feature set has the best cross validation results. Well will keep this the mind as the best model moving forward.

# Weighted Least Squares Regression

```{r}
# Shares Features
glm.share <- glm(log_salary ~ ., data= bdata.share)

glm.abs.res <- glm(abs(residuals(glm.share)) ~ fitted(glm.share))

wts.share <- 1/fitted(glm.abs.res)^2

bdata.share.wls <- bdata.share

wls.share <- glm(log_salary ~ ., data = bdata.share.wls, weights = wts.share)

bdata.share.wls$wts.share <- wts.share

cv.wls.share <- cv.glm(bdata.share.wls, wls.share)
```

```{r}
# VIF Features
glm.vif <- glm(log_salary ~ ., data = bdata.vif)

glm.abs.res <- glm(abs(residuals(glm.vif)) ~ fitted(glm.vif))

wts.vif <- 1/fitted(glm.abs.res)^2

bdata.vif.wls <- bdata.vif

wls.vif <- glm(log_salary ~ ., data = bdata.vif, weights = wts.vif)

bdata.vif.wls$wts.vif <- wts.vif

cv.wls.vif <- cv.glm(bdata.vif.wls, wls.vif)
```

```{r}
# Step Features

glm.step <- glm(log_salary ~ ., data = bdata.step)

glm.abs.res <- glm(abs(residuals(glm.step))~fitted(glm.step))

wts.step <- 1/fitted(glm.abs.res)^2

bdata.step.wls <- bdata.step

wls.step <- glm(log_salary ~ ., data = bdata.step, weights=wts.step)

bdata.step.wls$wts.step <- wts.step

cv.wls.step <- cv.glm(bdata.step.wls, wls.step)
```

## CV Results between Models and Feature Sets using Leave One Out

```{r}
cbind("WLS Share" = cv.wls.share$delta, "OLS Share" = cv.share$delta,
      "WLS VIF" = cv.wls.vif$delta, "OLS VIF" = cv.vif$delta,
      "WLS Step" = cv.wls.step$delta, "OLS Step" = cv.step$delta)
```

Traditional OLS Regression using the `Step` Features is still producing the best CV results

# Regression Trees

```{r}
library(tree)

tree.share <- tree(log_salary ~ ., bdata.share, mindev = 0.0005)

tree.vif <- tree(log_salary ~ ., bdata.vif, mindev = 0.0005)

tree.step <- tree(log_salary ~ ., bdata.step, mindev = 0.0005)

plot(tree.share)
text(tree.share)

plot(tree.vif)
text(tree.vif)

plot(tree.step)
text(tree.step)
```

Our trees are overly complicated. Let's prune the tree of each feature set to it's minimum deviance.

## Pruning Share Tree

```{r}
cv.tree.share <- cv.tree(tree.share)

min.dev.share <- min(cv.tree.share$dev)

for (i in seq(length(cv.tree.share$dev))) {
  if (cv.tree.share$dev[i] == min.dev.share) {
    best.ind <- i
  }
}

best.size <- cv.tree.share$size[best.ind]

prune.share <- prune.tree(tree.share, best=best.size)

summary(prune.share)
plot(prune.share)
text(prune.share, pretty=0)
```

## Pruning VIF Tree

```{r}
cv.tree.vif <- cv.tree(tree.vif)

min.dev.vif <- min(cv.tree.vif$dev)

for (i in seq(length(cv.tree.vif$dev))) {
  if (cv.tree.vif$dev[i] == min.dev.vif) {
    best.ind <- i
  }
}

best.size <- cv.tree.vif$size[best.ind] 

prune.vif <- prune.tree(tree.vif, best=best.size)

summary(prune.vif)
plot(prune.vif)
text(prune.vif, pretty=0)
```

## Pruning Step Tree

```{r}
cv.tree.step <- cv.tree(tree.step)

min.dev.step <- min(cv.tree.step$dev)

for (i in seq(length(cv.tree.step$dev))) {
  if (cv.tree.step$dev[i] == min.dev.step) {
    best.ind <- i
  }
}

best.size <- cv.tree.step$size[best.ind]
best.size

prune.step <- prune.tree(tree.step, best=best.size)

summary(prune.step)
plot(prune.step)
text(prune.step, pretty=0)
```

## Tree CV Results
```{r}
cbind("Tree Share" = min.dev.share, "Tree VIF" = min.dev.vif, "Tree Step" = min.dev.step)
```

The `Step` Feature set gives us the best performing Regression Tree

# Using CV to Determine the Best model

So far, the best modeling results have been OLS using the Step Feature Set, and Regression Trees using the VIF Feature set.

## Compare Using Manual CV

Let's compare this 2 models using manual cross validation

```{r}
cv <- function(t, r, s = 1) {
  
  tree.results <- c()
  ols.results <- c()
  
  for (i in 1:r) {
    set.seed(s - i + 1)
    train = sample(1:nrow(bdata.share), t*nrow(bdata.share))
  
    new.tree.step <- tree(log_salary ~ ., bdata.step[train,])
    pred.tree <- predict(new.tree.step, newdata = bdata.step[-train,]) 
    step.test <- bdata.step[-train, "log_salary"]
    tree.cv <- mean(c(((pred.tree - step.test)^2)[[1]]))
  
    new.ols.step <- lm(log_salary ~ ., bdata.step[train,])
    pred.ols <- predict(new.ols.step, newdata = bdata.step[-train,])
    step.test <- bdata.step[-train, "log_salary"]
    ols.cv <- mean(c(((pred.ols - step.test)^2)[[1]]))
    
    tree.results <- c(tree.results, tree.cv)
    ols.results <- c(ols.results, ols.cv)
  }
  
  
  cbind("Tree" = mean(tree.results), "OLS" = mean(ols.results))
}

cv(0.7, 1000, 2000)
```

OLS has won out against regression trees here.

## Compare Using 10 Folds Cross Validatiion


```{r}
set.seed(500)

bdata.step.sample <- bdata.step[sample(nrow(bdata.step)),]

folds <- cut(seq(1,nrow(bdata.step.sample)),breaks=10,labels=FALSE)

cv.ols <- c()
cv.tree <- c()

for (i in 1:10) {
  testInd <- which(folds==i,arr.ind=TRUE)
  testData <- bdata.step.sample[testInd, ]
  trainData <- bdata.step.sample[-testInd, ]
  
  step.test <- testData[, "log_salary"]
  
  new.ols.step <- lm(log_salary ~ ., data = trainData)
  ols.pred <- predict(new.ols.step, newdata = testData)
  ols.cv <- mean(c(((ols.pred - step.test)^2)[[1]]))
  
  new.tree.step <- tree(log_salary ~ ., data = trainData)
  tree.pred <- predict(new.tree.step, newdata = testData)
  tree.cv <- mean(c(((tree.pred - step.test)^2)[[1]]))
  
  cv.ols <- c(cv.ols, ols.cv)
  cv.tree <- c(cv.tree, tree.cv)
}

cbind("OLS" = mean(cv.ols), "Tree" = mean(cv.tree))
```

Once again, the OLS model performs better than Regression Trees


# Feature Sets Summary

```{r}
print("Shared Features")
vars.share
print("Step Features")
vars.step
print("VIF Features")
vars.vif
```

# Conclusion

OLS with the Shared Feature set was the model with the best CV results

```{r}
summary(ols.step)

plot(ols.share, which = c(1, 2))
```

## Should We Remove the insignificant variables?

```{r}
bdata.step %>%
  select(-draft_pick, -Age, -G, -BLK) ->
bdata.step2

ols.step2 <- glm(log_salary ~ ., data = bdata.step2)

lm.step <- lm(log_salary ~ ., data = bdata.step)
lm.step2 <- lm(log_salary ~ ., data = bdata.step2)


anova(lm.step2, lm.step)

cv.ols.step2 <- cv.glm(bdata.step2, ols.step2)

cbind("Step" = cv.step$delta, "Step2" = cv.ols.step2$delta)
```

The ANOVA test gives us significant results, this indicates using the larger feature set.

Additionally, the larger data set cross validates better. We will keep the insignificant variables in.

# Final Model

```{r}
summary(ols.step)

plot(ols.step, which = c(1,2))
```






